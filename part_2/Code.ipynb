{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Objective:\n",
    "My objective is to use Python to construct an approximate nearest neighbors algorithm from scratch.\n",
    "I am using a datset of movie ratings (~100,000 ratings) found here: https://grouplens.org/datasets/movielens/.\n",
    "In addition to building these algorithms, I will experiment with making changes to the standard models in order to\n",
    "gain insight and facts into how it is working and/or improve the performance of the model via nonstandard techniques.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['userId', 'movieId', 'rating', 'timestamp'], ['1', '31', '2.5', '1260759144'], ['1', '1029', '3', '1260759179'], ['1', '1061', '3', '1260759182'], ['1', '1129', '2', '1260759185'], ['1', '1172', '4', '1260759205'], ['1', '1263', '2', '1260759151'], ['1', '1287', '2', '1260759187'], ['1', '1293', '2', '1260759148'], ['1', '1339', '3.5', '1260759125'], ['1', '1343', '2', '1260759131'], ['1', '1371', '2.5', '1260759135'], ['1', '1405', '1', '1260759203'], ['1', '1953', '4', '1260759191'], ['1', '2105', '4', '1260759139'], ['1', '2150', '3', '1260759194'], ['1', '2193', '2', '1260759198'], ['1', '2294', '2', '1260759108'], ['1', '2455', '2.5', '1260759113'], ['1', '2968', '1', '1260759200']]\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "load ratings data\n",
    "\"\"\"\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir(\"C:/Users/ramy_/Documents/Graduate School/Semester 1/Personalization Theory and Application/ml-latest-small/\")\n",
    "\n",
    "\n",
    "data = []\n",
    "\n",
    "with open('ratings.csv', 'rb') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    for i, row in enumerate(spamreader):\n",
    "        data.append(row)\n",
    "\n",
    "print (data[0:20])\n",
    "print (data[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['userId' 'movieId' 'rating']\n",
      "<bound method DataFrame.head of movieId     1    10   100 100017 100032 100034 100083 100106 100159 100163  \\\n",
      "userId                                                                       \n",
      "1        None  None  None   None   None   None   None   None   None   None   \n",
      "10       None  None  None   None   None   None   None   None   None   None   \n",
      "100         4  None  None   None   None   None   None   None   None   None   \n",
      "101      None  None  None   None   None   None   None   None   None   None   \n",
      "102      None  None  None   None   None   None   None   None   None   None   \n",
      "103      None  None  None   None   None   None   None   None   None   None   \n",
      "104      None  None  None   None   None   None   None   None   None   None   \n",
      "105      None   3.5  None   None   None   None   None   None   None   None   \n",
      "106         4  None  None   None   None   None   None   None   None   None   \n",
      "107      None  None  None   None   None   None   None   None   None   None   \n",
      "108      None     4  None   None   None   None   None   None   None   None   \n",
      "109      None  None  None   None   None   None   None   None   None   None   \n",
      "11       None  None  None   None   None   None   None   None   None   None   \n",
      "110      None     4  None   None   None   None   None   None   None   None   \n",
      "111      None   3.5  None   None   None   None   None   None   None   None   \n",
      "112         5  None  None   None   None   None   None   None   None   None   \n",
      "113      None     3  None   None   None   None   None   None   None   None   \n",
      "114      None     4  None   None   None   None   None   None   None   None   \n",
      "115      None  None  None   None   None   None   None   None   None   None   \n",
      "116      None  None  None   None   None   None   None   None   None   None   \n",
      "117      None  None  None   None   None   None   None   None   None   None   \n",
      "118      None  None  None   None   None   None   None   None   None   None   \n",
      "119         2  None  None   None   None   None   None   None   None   None   \n",
      "12       None  None  None   None   None   None   None   None   None   None   \n",
      "120       2.5  None  None   None   None   None   None   None   None   None   \n",
      "121         4     3  None   None   None   None   None   None   None   None   \n",
      "122         3     3  None   None   None   None   None   None   None   None   \n",
      "123      None  None  None   None   None   None   None   None   None   None   \n",
      "124       4.5  None  None   None   None   None   None   None   None   None   \n",
      "125         4  None  None   None   None   None   None   None   None   None   \n",
      "...       ...   ...   ...    ...    ...    ...    ...    ...    ...    ...   \n",
      "72        3.5  None  None   None   None   None   None   None   None   None   \n",
      "73          5     3  None   None   None   None   None   None      4   None   \n",
      "74       None     5  None   None   None   None   None   None   None   None   \n",
      "75          3  None  None   None   None   None   None   None   None   None   \n",
      "76       None  None  None   None   None   None   None   None   None   None   \n",
      "77          4     3  None   None   None   None   None   None   None   None   \n",
      "78       None     5  None   None   None   None   None   None   None   None   \n",
      "79          2  None  None   None   None   None   None   None   None   None   \n",
      "8        None  None  None   None   None   None   None   None   None   None   \n",
      "80       None     3     2   None   None   None   None   None   None   None   \n",
      "81       None  None  None   None   None   None   None   None   None   None   \n",
      "82       None     3  None   None   None   None   None   None   None   None   \n",
      "83       None  None  None   None   None   None   None   None   None   None   \n",
      "84        3.5  None  None   None   None   None   None   None   None   None   \n",
      "85       None     5  None   None   None   None   None   None   None   None   \n",
      "86          3  None  None   None   None   None   None   None   None   None   \n",
      "87          3  None  None   None   None   None   None   None   None   None   \n",
      "88       None  None  None   None   None   None   None   None   None   None   \n",
      "89          5  None  None   None   None   None   None   None   None   None   \n",
      "9           4  None  None   None   None   None   None   None   None   None   \n",
      "90          4  None  None   None   None   None   None   None   None   None   \n",
      "91          5  None  None   None   None   None   None   None   None   None   \n",
      "92          5     2  None   None   None   None   None   None   None   None   \n",
      "93          4  None  None   None   None   None   None   None   None   None   \n",
      "94          4  None  None   None   None   None   None   None   None   None   \n",
      "95       None  None  None   None   None   None   None   None   None   None   \n",
      "96       None  None  None   None   None   None   None   None   None   None   \n",
      "97          1  None  None   None   None   None   None   None   None   None   \n",
      "98       None  None  None   None   None   None   None   None   None   None   \n",
      "99          4  None  None   None   None   None   None   None   None   None   \n",
      "\n",
      "movieId  ...  99795   998 99811 99813 99839 99846   999 99912 99917 99992  \n",
      "userId   ...                                                               \n",
      "1        ...   None  None  None  None  None  None  None  None  None  None  \n",
      "10       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "100      ...   None  None  None  None  None  None  None  None  None  None  \n",
      "101      ...   None  None  None  None  None  None  None  None  None  None  \n",
      "102      ...   None  None  None  None  None  None  None  None  None  None  \n",
      "103      ...   None  None  None  None  None  None  None  None  None  None  \n",
      "104      ...   None  None  None  None  None  None  None  None  None  None  \n",
      "105      ...   None  None  None  None  None  None  None  None  None  None  \n",
      "106      ...   None  None  None  None  None  None  None  None  None  None  \n",
      "107      ...   None  None  None  None  None  None  None  None  None  None  \n",
      "108      ...   None  None  None  None  None  None  None  None  None  None  \n",
      "109      ...   None  None  None  None  None  None  None  None  None  None  \n",
      "11       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "110      ...   None  None  None  None  None  None  None  None  None  None  \n",
      "111      ...   None  None  None  None  None  None  None  None  None  None  \n",
      "112      ...   None  None  None  None  None  None  None  None  None  None  \n",
      "113      ...   None  None  None  None  None  None  None  None  None  None  \n",
      "114      ...   None  None  None  None  None  None  None  None  None  None  \n",
      "115      ...   None  None  None  None  None  None  None  None  None  None  \n",
      "116      ...   None  None  None  None  None  None  None  None  None  None  \n",
      "117      ...   None  None  None  None  None  None  None  None  None  None  \n",
      "118      ...   None  None  None  None  None  None  None  None  None  None  \n",
      "119      ...   None  None  None  None  None  None  None  None  None  None  \n",
      "12       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "120      ...   None  None  None  None  None  None  None  None  None  None  \n",
      "121      ...   None  None  None  None  None  None  None  None  None  None  \n",
      "122      ...   None  None  None  None  None  None  None  None  None  None  \n",
      "123      ...   None  None  None  None  None  None  None  None  None  None  \n",
      "124      ...   None  None  None  None  None  None  None  None  None  None  \n",
      "125      ...   None  None  None  None  None  None  None  None  None  None  \n",
      "...      ...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
      "72       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "73       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "74       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "75       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "76       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "77       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "78       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "79       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "8        ...   None  None  None  None  None  None  None  None  None  None  \n",
      "80       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "81       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "82       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "83       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "84       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "85       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "86       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "87       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "88       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "89       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "9        ...   None  None  None  None  None  None  None  None  None  None  \n",
      "90       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "91       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "92       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "93       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "94       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "95       ...   None  None  None  None  None  None     4  None  None  None  \n",
      "96       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "97       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "98       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "99       ...   None  None  None  None  None  None  None  None  None  None  \n",
      "\n",
      "[671 rows x 9066 columns]>\n",
      "(671, 9066)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "put data into proper format: pivot table by doing grouby on userID columns\n",
    "\"\"\"\n",
    "data = np.asarray(data)\n",
    "data = pd.DataFrame(data=data[1:,:], columns=data[0,0:])\n",
    "\n",
    "data = data.drop('timestamp', 1)\n",
    "\n",
    "#print (data.head)\n",
    "print (data.columns.values)\n",
    "\n",
    "data = data.pivot(index='userId', columns='movieId')['rating']\n",
    "\n",
    "print (data.head)\n",
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.64391416087\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "calculate sparsity\n",
    "\"\"\"\n",
    "total = 0\n",
    "rated = 0\n",
    "for i in range(data.shape[0]):\n",
    "    for j in range(data.shape[1]):\n",
    "        total += 1\n",
    "        if data.iloc[i, j] != None:\n",
    "            rated += 1\n",
    "\n",
    "print ((float(rated)/total)*100)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(671L, 9066L)\n",
      "(100L, 9066L)\n",
      "[[None None None ..., None None None]\n",
      " [None '4' None ..., None None None]\n",
      " [None None None ..., None None None]\n",
      " ..., \n",
      " [None '3' None ..., None None None]\n",
      " [None None None ..., None None None]\n",
      " [None None None ..., None None None]]\n",
      "(571L, 9066L)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "create training and evaluation sets\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "\n",
    "evaluation_set_size = 100\n",
    "\n",
    "initial_indices = []\n",
    "for i in range(data.shape[0]):\n",
    "    initial_indices.append(i)\n",
    "\n",
    "indices = []\n",
    "while len(indices) < evaluation_set_size:\n",
    "    number = random.choice(initial_indices)\n",
    "    if number not in indices:\n",
    "        indices.append(number)\n",
    "\n",
    "data = np.asarray(data)\n",
    "print (data.shape)\n",
    "eval = []\n",
    "for i in range(data.shape[0]):\n",
    "    if i in indices:\n",
    "        eval.append(data[i][:])\n",
    "eval = np.asarray(eval)\n",
    "print (eval.shape)\n",
    "print (eval)\n",
    "data = np.delete(data, indices, axis=0)\n",
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "strip off ratings from the evaluation set\n",
    "\"\"\"\n",
    "import copy\n",
    "\n",
    "ratio_of_ratings_missing = 0.5\n",
    "\n",
    "full_ratings_eval = copy.deepcopy(eval)\n",
    "for j, row in enumerate(eval):\n",
    "    indices = []\n",
    "    for i, rating in enumerate(row):\n",
    "        if rating != None:\n",
    "            indices.append(i)\n",
    "    total_ratings = len(indices)\n",
    "    while len(indices) > total_ratings*ratio_of_ratings_missing:\n",
    "        number = random.choice(indices)\n",
    "        indices.remove(number)\n",
    "    for i, rating in enumerate(row):\n",
    "        if i in indices:\n",
    "            eval[j][i] = None\n",
    "            \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "identify k nearest neighbors of every user in the evaluation set\n",
    "\"\"\"\n",
    "\n",
    "k = 5\n",
    "\n",
    "dictionaries = [None] * evaluation_set_size\n",
    "for i, eval_row in enumerate(eval):\n",
    "    dictionaries[i] = {}\n",
    "    for ref_row in data:\n",
    "        measure = 0\n",
    "        count = 0\n",
    "        for j in range(len(eval_row)):\n",
    "            if eval_row[j] != None and ref_row[j] != None:\n",
    "                count += 1\n",
    "                measure += (float(eval_row[j]) - float(ref_row[j]))**2\n",
    "        if len(dictionaries[i]) < 5 and count>0:\n",
    "            #print (measure)\n",
    "            dictionaries[i][tuple(ref_row.tolist())] = measure\n",
    "        else:\n",
    "            if count>0 and measure < max(dictionaries[i].values()):\n",
    "                #print (\"measure\")\n",
    "                #print (dictionaries[i].values())\n",
    "                #print (measure)\n",
    "                #print (max(dictionaries[i].values()))\n",
    "                for key in list(dictionaries[i].keys()):\n",
    "                    if dictionaries[i][key] == max(dictionaries[i].values()):\n",
    "                        del dictionaries[i][key]\n",
    "                        break\n",
    "                dictionaries[i][tuple(ref_row.tolist())] = measure\n",
    "                #print (dictionaries[i].values())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "make predictions on missing ratings based on nearest neighbors\n",
    "\"\"\"\n",
    "\n",
    "record = copy.deepcopy(eval)\n",
    "\n",
    "for i, eval_row in enumerate(eval):\n",
    "    for j in range(len(eval_row)):\n",
    "        if eval_row[j] == None and full_ratings_eval[i][j] != None:\n",
    "            total = 0\n",
    "            the_sum = 0\n",
    "            for v in dictionaries[i].keys():\n",
    "                if v[j] != None:\n",
    "                    the_sum += float(v[j])\n",
    "                    total += 1\n",
    "            if total == 0:\n",
    "                eval[i][j] = 3\n",
    "            else:\n",
    "                eval[i][j] = float(the_sum)/float(total)\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9498\n",
      "0.960047729347\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "calculate standard error among predicted ratings\n",
    "\"\"\"\n",
    "\n",
    "count = 0\n",
    "error = 0\n",
    "for i in range(len(eval)):\n",
    "    for j in range(len(eval[i])):\n",
    "        if eval[i][j] != None and record[i][j] == None:\n",
    "            count += 1\n",
    "            error += abs(float(full_ratings_eval[i][j]) - float(eval[i][j]))\n",
    "            #print (\"original\")\n",
    "            #print (full_ratings_eval[i][j])\n",
    "            #print (eval[i][j])\n",
    "print (count)\n",
    "print (float(error)/float(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287\n",
      "0\n",
      "9066\n",
      "8\n",
      "41.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "calculate popularity of movies in dataset\n",
    "\"\"\"\n",
    "counts = [0] * data.shape[1]\n",
    "for i in range(data.shape[0]):\n",
    "    for j in range(data.shape[1]):\n",
    "        if data[i][j] != None:\n",
    "            counts[j] += 1\n",
    "\n",
    "print (max(counts))\n",
    "print (min(counts))\n",
    "print (len(counts))\n",
    "print (sum(counts)/len(counts))\n",
    "print (np.percentile(counts, 95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4502\n",
      "0.902765437583\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "calculate standard error among predicted ratings only among a subset of predictions\n",
    "based on whether the movie whose rating is being predicted is popular or not\n",
    "\"\"\"\n",
    "count = 0\n",
    "error = 0\n",
    "for i in range(len(eval)):\n",
    "    for j in range(len(eval[i])):\n",
    "        if eval[i][j] != None and record[i][j] == None:\n",
    "            if counts[j] <= np.percentile(counts, 90):\n",
    "                count += 1\n",
    "                error += abs(float(full_ratings_eval[i][j]) - float(eval[i][j]))\n",
    "            #print (\"original\")\n",
    "            #print (full_ratings_eval[i][j])\n",
    "            #print (eval[i][j])\n",
    "print (count)\n",
    "print (float(error)/float(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1196\n",
      "10\n",
      "100\n",
      "95\n",
      "44.5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "calculate how prolific users are in evaluation set\n",
    "\"\"\"\n",
    "users = [0] * record.shape[0]\n",
    "for i in range(record.shape[0]):\n",
    "    for j in range(record.shape[1]):\n",
    "        if record[i][j] != None:\n",
    "            users[i] += 1\n",
    "\n",
    "print (max(users))\n",
    "print (min(users))\n",
    "print (len(users))\n",
    "print (sum(users)/len(users))\n",
    "print (np.percentile(users, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4783\n",
      "0.91642274723\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "calculate standard error among predicted ratings only among a subset of predictions\n",
    "based on how prolific the user who made the predictions is\n",
    "\"\"\"\n",
    "count = 0\n",
    "error = 0\n",
    "for i in range(len(eval)):\n",
    "    for j in range(len(eval[i])):\n",
    "        if eval[i][j] != None and record[i][j] == None:\n",
    "            if users[i] > np.percentile(users, 90):\n",
    "                count += 1\n",
    "                error += abs(float(full_ratings_eval[i][j]) - float(eval[i][j]))\n",
    "            #print (\"original\")\n",
    "            #print (full_ratings_eval[i][j])\n",
    "            #print (eval[i][j])\n",
    "print (count)\n",
    "print (float(error)/float(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Results, insights and discussion\n",
    "---------------------------------\n",
    "\n",
    "Benchmark:\n",
    "data: 100k ratings, 9k movies, 671 users\n",
    "sparsity (percentage of ratings of movies present in datasest): 1.64%\n",
    "train/test split: 571 users for training set, 100 users for evaluation set\n",
    "level of knowledge on evaluation set: 50% of user's ratings missing\n",
    "value of k for knn: 5\n",
    "default guess (if none of user's neighbors have rated a movie): 3 out of 5\n",
    "\n",
    "Benchmark results:\n",
    "total number of predictions made: 9498\n",
    "standard error from true rating: 0.96 out of 5\n",
    "\n",
    "Insights:\n",
    "among top 10% popular movies (by frequency of ratings):\n",
    "total number of predictions made: 4996\n",
    "standard error from true rating: 1.01 out of 5\n",
    "\n",
    "among bottom 90% popular movies (by frequency of ratings):\n",
    "total number of predictions made: 4502\n",
    "standard error from true rating: 0.902 out of 5\n",
    "\n",
    "\n",
    "The above observation is that if a movie is more popular, the algorithm will have a slightly higher error making\n",
    "a prediction on it, while if a movie is less popular, the algorithm will have a lower error making a prediction on it.\n",
    "This is due to the fact that in a sense there are more widespread opinions about popular movies. If a user agrees\n",
    "with another user on particular type of unpopular movie, that does not necessarily tell us that they will share\n",
    "a similar opinion on the movie that everyone is familiar with.\n",
    "It is also the case that with unpopular movies, the error rates were better. This is due to the fact that unpopular\n",
    "movies can more easily fit into a particular, possibly esoteric, genre. If users are found to be similiar based\n",
    "on their ratings, then we can be more sure that when it comes to unpopular movies, they are more likely to have\n",
    "a similar opinion.\n",
    "This observation based on popularity is a useful observation in a business setting. Our model is more powerful\n",
    "if we use it on the half of the data that constitute \"unpopular\" movies.\n",
    "\n",
    "\n",
    "within top 50% of users ranked by how many movies they have rated:\n",
    "total number of predictions made: 8330\n",
    "standard error from true rating: 0.946 out of 5\n",
    "\n",
    "\n",
    "within bottom 50% of users ranked by how many movies they have rated:\n",
    "total number of predictions made: 1168\n",
    "standard error from true rating: 1.059 out of 5\n",
    "\n",
    "We observe that if a user is more prolific, that is, he or she has rated more movies, the standard error for predictions\n",
    "on his or her ratings is slightly lower. And if a user is less prolific, the standard error on his or her predictions\n",
    "is slightly higher. This is certainly expected. This phenomenon is due to the fact that if a user has rated\n",
    "more movies, the algorithm has more information about him or her preferences: the algorithm can match \"nearest\n",
    "neighbors\" to him or her more accurately. If the nearest neighbors to a user are matched accurately, the predictions\n",
    "made on the ratings will have less error.\n",
    "\n",
    "Additional segmentation based on users:\n",
    "within top 90% of users ranked by how many movies they have rated:\n",
    "total number of predictions made: 4783\n",
    "standard error from true rating: 0.916 out of 5\n",
    "\n",
    "From a business perspective, this observation gives us information that users who are more prolific will obtain\n",
    "better predictions. We may want to act by deploying our model on only the more prolific users. Additionally,\n",
    "as a business it helps to create incentives for users to give more ratings, which can be done in a variety of ways.\n",
    "Some ways include prompting the user to rate a movie rather than passively waiting for a rating, and actively\n",
    "informing the user that giving more ratings leads to better recommendations. Other options can be related to\n",
    "the UX design of the rating mechanism. It is best if the design is optimal for engagement.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load movie genre data\n",
    "\"\"\"\n",
    "\n",
    "original = []\n",
    "\n",
    "with open('movies.csv', 'rb') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    for i, row in enumerate(spamreader):\n",
    "        original.append(row)\n",
    "genres = {}\n",
    "for i in range(len(original)):\n",
    "    try:\n",
    "        genres[original[i][0]] = original[i][5].split(\"|\")[0]\n",
    "    except:\n",
    "        try:\n",
    "            genres[original[i][0]] = original[i][4].split(\"|\")[0]\n",
    "        except:\n",
    "            try:\n",
    "                genres[original[i][0]] = original[i][3].split(\"|\")[0]\n",
    "            except:\n",
    "                genres[original[i][0]] = original[i][2].split(\"|\")[0]\n",
    "#print (genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1' '10' '100' ..., '99912' '99917' '99992']\n",
      " [None None None ..., None None None]\n",
      " [None None None ..., None None None]\n",
      " ..., \n",
      " ['1' None None ..., None None None]\n",
      " [None None None ..., None None None]\n",
      " ['4' None None ..., None None None]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "put data into proper format: pivot table by doing grouby on userID columns\n",
    "and append movieID row\n",
    "\"\"\"\n",
    "data = []\n",
    "\n",
    "with open('ratings.csv', 'rb') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    for i, row in enumerate(spamreader):\n",
    "        data.append(row)\n",
    "\n",
    "data = np.asarray(data)\n",
    "data = pd.DataFrame(data=data[1:,:], columns=data[0,0:])\n",
    "\n",
    "data = data.drop('timestamp', 1)\n",
    "\n",
    "#print (data.head)\n",
    "\n",
    "#print (data.head)\n",
    "#print (data.columns.values)\n",
    "\n",
    "data = data.pivot(index='userId', columns='movieId', values='rating')#['rating']\n",
    "\n",
    "#print (data.iloc[2,:])\n",
    "#print (data.columns[:])\n",
    "#print (len(data.columns[:]))\n",
    "\n",
    "cols = data.columns[:]\n",
    "\n",
    "data = np.asarray(data)\n",
    "\n",
    "\n",
    "\n",
    "data = np.vstack((cols,data))\n",
    "\n",
    "print (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "671\n",
      "set(['Horror', 'Crime', 'Drama', 'Adventure', 'Action', 'Comedy'])\n",
      "[[None None None ..., None None 30]\n",
      " [None None None ..., None None 50]\n",
      " ['4' None None ..., None None 50]\n",
      " ..., \n",
      " ['1' None None ..., None None 60]\n",
      " [None None None ..., None None 60]\n",
      " ['4' None None ..., None None 30]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "append column of most watched movie genre to dataset\n",
    "\"\"\"\n",
    "vector = []\n",
    "for i in range(1, len(data)):\n",
    "    temp = []\n",
    "    for j in range(len(data[i])):\n",
    "        if data[i][j] != None:\n",
    "            temp.append(data[0][j])\n",
    "    for k in range(len(temp)):\n",
    "        t = temp[k]\n",
    "        a = genres[t]\n",
    "        temp[k] = a\n",
    "    mode = max(set(temp), key=temp.count)\n",
    "    vector.append(mode)\n",
    "\n",
    "print (len(vector))\n",
    "#print (vector)\n",
    "\n",
    "print (set(vector))\n",
    "\n",
    "for i in range(len(vector)):\n",
    "    if vector[i] == 'Horror':\n",
    "        vector[i] = 10\n",
    "    if vector[i] == 'Crime':\n",
    "        vector[i] = 20\n",
    "    if vector[i] == 'Drama':\n",
    "        vector[i] = 30\n",
    "    if vector[i] == 'Adventure':\n",
    "        vector[i] = 40\n",
    "    if vector[i] == 'Action':\n",
    "        vector[i] = 50\n",
    "    if vector[i] == 'Comedy':\n",
    "        vector[i] = 60\n",
    "#print (vector)\n",
    "\n",
    "data = np.delete(data, 0, 0)\n",
    "\n",
    "vector = np.asarray(vector)\n",
    "vector = np.reshape(vector, (len(vector), 1))\n",
    "\n",
    "data = np.append(data, vector, axis=1)\n",
    "\n",
    "\n",
    "print (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(671L, 9067L)\n",
      "(100L, 9067L)\n",
      "[[None None None ..., None None 50]\n",
      " [None None None ..., None None 30]\n",
      " ['4' None None ..., None None 60]\n",
      " ..., \n",
      " ['3' None None ..., None None 50]\n",
      " ['4' '3' None ..., None None 50]\n",
      " ['4' None None ..., None None 30]]\n",
      "(571L, 9067L)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "create training and evaluation sets\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "\n",
    "evaluation_set_size = 100\n",
    "\n",
    "initial_indices = []\n",
    "for i in range(data.shape[0]):\n",
    "    initial_indices.append(i)\n",
    "\n",
    "indices = []\n",
    "while len(indices) < evaluation_set_size:\n",
    "    number = random.choice(initial_indices)\n",
    "    if number not in indices:\n",
    "        indices.append(number)\n",
    "\n",
    "data = np.asarray(data)\n",
    "print (data.shape)\n",
    "eval = []\n",
    "for i in range(data.shape[0]):\n",
    "    if i in indices:\n",
    "        eval.append(data[i][:])\n",
    "eval = np.asarray(eval)\n",
    "print (eval.shape)\n",
    "print (eval)\n",
    "data = np.delete(data, indices, axis=0)\n",
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "strip off ratings from the evaluation set\n",
    "\"\"\"\n",
    "import copy\n",
    "\n",
    "ratio_of_ratings_missing = 0.5\n",
    "\n",
    "full_ratings_eval = copy.deepcopy(eval)\n",
    "for j, row in enumerate(eval):\n",
    "    indices = []\n",
    "    for i, rating in enumerate(row):\n",
    "        if rating != None and i!=9066:\n",
    "            indices.append(i)\n",
    "    total_ratings = len(indices)\n",
    "    while len(indices) > total_ratings*ratio_of_ratings_missing:\n",
    "        number = random.choice(indices)\n",
    "        indices.remove(number)\n",
    "    for i, rating in enumerate(row):\n",
    "        if i in indices:\n",
    "            eval[j][i] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "identify k nearest neighbors of every user in the evaluation set\n",
    "\"\"\"\n",
    "\n",
    "k = 5\n",
    "\n",
    "dictionaries = [None] * evaluation_set_size\n",
    "for i, eval_row in enumerate(eval):\n",
    "    dictionaries[i] = {}\n",
    "    for ref_row in data:\n",
    "        measure = 0\n",
    "        count = 0\n",
    "        for j in range(len(eval_row)):\n",
    "            if eval_row[j] != None and ref_row[j] != None:\n",
    "                count += 1\n",
    "                measure += (float(eval_row[j]) - float(ref_row[j]))**2\n",
    "        if len(dictionaries[i]) < 5 and count>0:\n",
    "            #print (measure)\n",
    "            dictionaries[i][tuple(ref_row.tolist())] = measure\n",
    "        else:\n",
    "            if count>0 and measure < max(dictionaries[i].values()):\n",
    "                #print (\"measure\")\n",
    "                #print (dictionaries[i].values())\n",
    "                #print (measure)\n",
    "                #print (max(dictionaries[i].values()))\n",
    "                for key in list(dictionaries[i].keys()):\n",
    "                    if dictionaries[i][key] == max(dictionaries[i].values()):\n",
    "                        del dictionaries[i][key]\n",
    "                        break\n",
    "                dictionaries[i][tuple(ref_row.tolist())] = measure\n",
    "                #print (dictionaries[i].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "make predictions on missing ratings based on nearest neighbors\n",
    "\"\"\"\n",
    "\n",
    "record = copy.deepcopy(eval)\n",
    "\n",
    "for i, eval_row in enumerate(eval):\n",
    "    for j in range(len(eval_row)):\n",
    "        if eval_row[j] == None and full_ratings_eval[i][j] != None:\n",
    "            total = 0\n",
    "            the_sum = 0\n",
    "            for v in dictionaries[i].keys():\n",
    "                if v[j] != None:\n",
    "                    the_sum += float(v[j])\n",
    "                    total += 1\n",
    "            if total == 0:\n",
    "                eval[i][j] = 3\n",
    "            else:\n",
    "                eval[i][j] = float(the_sum)/float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7730\n",
      "0.914505174644\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "calculate standard error among predicted ratings\n",
    "\"\"\"\n",
    "\n",
    "count = 0\n",
    "error = 0\n",
    "for i in range(len(eval)):\n",
    "    for j in range(len(eval[i])):\n",
    "        if eval[i][j] != None and record[i][j] == None:\n",
    "            count += 1\n",
    "            error += abs(float(full_ratings_eval[i][j]) - float(eval[i][j]))\n",
    "            #print (\"original\")\n",
    "            #print (full_ratings_eval[i][j])\n",
    "            #print (eval[i][j])\n",
    "print (count)\n",
    "print (float(error)/float(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In my experiment I achieved a better working model by adding side information. I used data provided by\n",
    "https://grouplens.org/datasets/movielens/ which gives the genre for each movie in the dataset. There could\n",
    "be many ways to incorporate this information and it is tricky to do so properly. My method was as follows:\n",
    "For each user, I computed the most common movie genre that they liked to watch. Then, I added a column in the\n",
    "dataset, that is, a new feature for each user, and it described each user's most watched genre. In order to work\n",
    "with nearest neighbors method, these values had to be numerical, so I used a simple dictionary conversion for each\n",
    "genre to a number -- and I made the numbers relatively large (e.g. 10, 20, 30) compared to the ratings. This\n",
    "makes the feature stick out. To be precise, differences between users' most watched genres will mean that they\n",
    "are very different users in the algorithm, and vice versa. So as you compare users based on ratings of movies watched,\n",
    "you likewise compare users based on which genre they watch the most. Thus your nearest neighbors are collected.\n",
    "\n",
    "\n",
    "Improvement on benchmark by adding side information on genre:\n",
    "total number of predictions made: 7730\n",
    "standard error from true rating: 0.914 out of 5\n",
    "\n",
    "\n",
    "In summary, I have built a custom nearest neighbors algorithm from scratch. My insights on the performance relate\n",
    "to the activity of users and the characteristics of movies. And they make logical, intuitive sense and can be\n",
    "used to back business opinions as well as motivate business decisions. The improvement I discovered uses\n",
    "side information about the movie's genre, offered by the source website. Adding a favorite genre user information\n",
    "was beneficial to the model.\n",
    "\n",
    "This concludes the model building and experimentation.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
